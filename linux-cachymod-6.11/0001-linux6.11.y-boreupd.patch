# Update patch for BORE 5.7.10
* Backport linux6.12.y-bore5.9.5

diff -uarp a/include/linux/sched/bore.h b/include/linux/sched/bore.h
--- a/include/linux/sched/bore.h
+++ b/include/linux/sched/bore.h
@@ -4,7 +4,7 @@
 
 #ifndef _LINUX_SCHED_BORE_H
 #define _LINUX_SCHED_BORE_H
-#define SCHED_BORE_VERSION "5.7.10"
+#define SCHED_BORE_VERSION "5.9.5"
 
 #ifdef CONFIG_SCHED_BORE
 extern u8   __read_mostly sched_bore;
@@ -26,12 +26,12 @@ extern void restart_burst(struct sched_e
 extern void restart_burst_rescale_deadline(struct sched_entity *se);
 
 extern int sched_bore_update_handler(const struct ctl_table *table, int write,
-		void __user *buffer, size_t *lenp, loff_t *ppos);
+	void __user *buffer, size_t *lenp, loff_t *ppos);
 
 extern void sched_clone_bore(
-	struct task_struct *p, struct task_struct *parent, u64 clone_flags);
+	struct task_struct *p, struct task_struct *parent, u64 clone_flags, u64 now);
 
-extern void init_task_bore(struct task_struct *p);
+extern void reset_task_bore(struct task_struct *p);
 extern void sched_bore_init(void);
 
 extern void reweight_entity(
diff -uarp a/include/linux/sched.h b/include/linux/sched.h
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -540,6 +540,7 @@ struct sched_burst_cache {
 	u8				score;
 	u32				count;
 	u64				timestamp;
+	spinlock_t			lock;
 };
 #endif // CONFIG_SCHED_BORE
 
diff -uarp a/kernel/fork.c b/kernel/fork.c
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -2365,9 +2365,6 @@ __latent_entropy struct task_struct *cop
 	retval = sched_fork(clone_flags, p);
 	if (retval)
 		goto bad_fork_cleanup_policy;
-#ifdef CONFIG_SCHED_BORE
-	sched_clone_bore(p, current, clone_flags);
-#endif // CONFIG_SCHED_BORE
 
 	retval = perf_event_init_task(p, clone_flags);
 	if (retval)
@@ -2517,6 +2514,10 @@ __latent_entropy struct task_struct *cop
 	p->start_time = ktime_get_ns();
 	p->start_boottime = ktime_get_boottime_ns();
 
+#ifdef CONFIG_SCHED_BORE
+	if (likely(p->pid))
+		sched_clone_bore(p, current, clone_flags, p->start_time);
+#endif // CONFIG_SCHED_BORE
 	/*
 	 * Make it visible to the rest of the system, but dont wake it up yet.
 	 * Need tasklist lock for parent etc handling!
diff -uarp a/kernel/sched/bore.c b/kernel/sched/bore.c
--- a/kernel/sched/bore.c
+++ b/kernel/sched/bore.c
@@ -121,7 +121,7 @@ void restart_burst_rescale_deadline(stru
 }
 
 static inline bool task_is_bore_eligible(struct task_struct *p)
-{return p->sched_class == &fair_sched_class;}
+{return p && p->sched_class == &fair_sched_class && !p->exit_state;}
 
 static void reset_task_weights_bore(void) {
 	struct task_struct *task;
@@ -132,15 +132,16 @@ static void reset_task_weights_bore(void
 	for_each_process(task) {
 		if (!task_is_bore_eligible(task)) continue;
 		rq = task_rq(task);
-		rq_lock_irqsave(rq, &rf);
+		rq_pin_lock(rq, &rf);
+		update_rq_clock(rq);
 		reweight_task_by_prio(task, effective_prio(task));
-		rq_unlock_irqrestore(rq, &rf);
+		rq_unpin_lock(rq, &rf);
 	}
 	write_unlock_irq(&tasklist_lock);
 }
 
 int sched_bore_update_handler(const struct ctl_table *table, int write,
-		void __user *buffer, size_t *lenp, loff_t *ppos) {
+	void __user *buffer, size_t *lenp, loff_t *ppos) {
 	int ret = proc_dou8vec_minmax(table, write, buffer, lenp, ppos);
 	if (ret || !write)
 		return ret;
@@ -153,18 +154,21 @@ int sched_bore_update_handler(const stru
 #define for_each_child(p, t) \
 	list_for_each_entry(t, &(p)->children, sibling)
 
-static u32 count_children_max2(struct task_struct *p) {
-	u32 cnt = 0;
-	struct task_struct *child;
-	for_each_child(p, child) {if (2 <= ++cnt) break;}
-	return cnt;
+static u32 count_entries_upto2(struct list_head *head) {
+	struct list_head *next = head->next;
+	return (next != head) + (next->next != head);
+}
+
+static inline void init_task_burst_cache_lock(struct task_struct *p) {
+	spin_lock_init(&p->se.child_burst.lock);
+	spin_lock_init(&p->se.group_burst.lock);
 }
 
 static inline bool burst_cache_expired(struct sched_burst_cache *bc, u64 now)
 {return (s64)(bc->timestamp + sched_burst_cache_lifetime - now) < 0;}
 
 static void update_burst_cache(struct sched_burst_cache *bc,
-		struct task_struct *p, u32 cnt, u32 sum, u64 now) {
+	struct task_struct *p, u32 cnt, u32 sum, u64 now) {
 	u8 avg = cnt ? sum / cnt : 0;
 	bc->score = max(avg, p->se.burst_penalty);
 	bc->count = cnt;
@@ -184,22 +188,32 @@ static inline void update_child_burst_di
 	update_burst_cache(&p->se.child_burst, p, cnt, sum, now);
 }
 
-static inline u8 inherit_burst_direct(struct task_struct *p, u64 now) {
+static inline u8 inherit_burst_direct(
+	struct task_struct *p, u64 now, u64 clone_flags) {
 	struct task_struct *parent = p;
-	if (burst_cache_expired(&parent->se.child_burst, now))
+	struct sched_burst_cache *bc;
+
+	if (clone_flags & CLONE_PARENT)
+		parent = parent->real_parent;
+
+	bc = &parent->se.child_burst;
+	spin_lock(&bc->lock);
+	if (burst_cache_expired(bc, now))
 		update_child_burst_direct(parent, now);
+	spin_unlock(&bc->lock);
 
-	return parent->se.child_burst.score;
+	return bc->score;
 }
 
 static void update_child_burst_topological(
 	struct task_struct *p, u64 now, u32 depth, u32 *acnt, u32 *asum) {
 	u32 cnt = 0, dcnt = 0, sum = 0;
 	struct task_struct *child, *dec;
+	struct sched_burst_cache *bc __maybe_unused;
 
 	for_each_child(p, child) {
 		dec = child;
-		while ((dcnt = count_children_max2(dec)) == 1)
+		while ((dcnt = count_entries_upto2(&dec->children)) == 1)
 			dec = list_first_entry(&dec->children, struct task_struct, sibling);
 		
 		if (!dcnt || !depth) {
@@ -208,13 +222,20 @@ static void update_child_burst_topologic
 			sum += dec->se.burst_penalty;
 			continue;
 		}
-		if (!burst_cache_expired(&dec->se.child_burst, now)) {
-			cnt += dec->se.child_burst.count;
-			sum += (u32)dec->se.child_burst.score * dec->se.child_burst.count;
-			if (sched_burst_cache_stop_count <= cnt) break;
+		bc = &dec->se.child_burst;
+		spin_lock(&bc->lock);
+		if (!burst_cache_expired(bc, now)) {
+			cnt += bc->count;
+			sum += (u32)bc->score * bc->count;
+			if (sched_burst_cache_stop_count <= cnt) {
+				spin_unlock(&bc->lock);
+				break;
+			}
+			spin_unlock(&bc->lock);
 			continue;
 		}
 		update_child_burst_topological(dec, now, depth - 1, &cnt, &sum);
+		spin_unlock(&bc->lock);
 	}
 
 	update_burst_cache(&p->se.child_burst, p, cnt, sum, now);
@@ -222,19 +243,33 @@ static void update_child_burst_topologic
 	*asum += sum;
 }
 
-static inline u8 inherit_burst_topological(struct task_struct *p, u64 now) {
+static inline u8 inherit_burst_topological(
+	struct task_struct *p, u64 now, u64 clone_flags) {
 	struct task_struct *anc = p;
+	struct sched_burst_cache *bc;
 	u32 cnt = 0, sum = 0;
+	u32 base_child_cnt = 0;
+
+	if (clone_flags & CLONE_PARENT) {
+		anc = anc->real_parent;
+		base_child_cnt = 1;
+	}
 
 	for (struct task_struct *next;
-		 anc != (next = anc->real_parent) && count_children_max2(anc) <= 1;
-		 anc = next) {}
+		 anc != (next = anc->real_parent) &&
+			count_entries_upto2(&anc->children) <= base_child_cnt;) {
+		anc = next;
+		base_child_cnt = 1;
+	}
 
-	if (burst_cache_expired(&anc->se.child_burst, now))
+	bc = &anc->se.child_burst;
+	spin_lock(&bc->lock);
+	if (burst_cache_expired(bc, now))
 		update_child_burst_topological(
 			anc, now, sched_burst_fork_atavistic - 1, &cnt, &sum);
+	spin_unlock(&bc->lock);
 
-	return anc->se.child_burst.score;
+	return bc->score;
 }
 
 static inline void update_tg_burst(struct task_struct *p, u64 now) {
@@ -251,34 +286,37 @@ static inline void update_tg_burst(struc
 }
 
 static inline u8 inherit_burst_tg(struct task_struct *p, u64 now) {
-	struct task_struct *parent = p->group_leader;
-	if (burst_cache_expired(&parent->se.group_burst, now))
+	struct task_struct *parent = rcu_dereference(p->group_leader);
+	struct sched_burst_cache *bc = &parent->se.group_burst;
+	spin_lock(&bc->lock);
+	if (burst_cache_expired(bc, now))
 		update_tg_burst(parent, now);
+	spin_unlock(&bc->lock);
 
-	return parent->se.group_burst.score;
+	return bc->score;
 }
 
-void sched_clone_bore(
-	struct task_struct *p, struct task_struct *parent, u64 clone_flags) {
-	u64 now;
+void sched_clone_bore(struct task_struct *p,
+	struct task_struct *parent, u64 clone_flags, u64 now) {
+	struct sched_entity *se = &p->se;
 	u8 penalty;
 
+	init_task_burst_cache_lock(p);
+
 	if (!task_is_bore_eligible(p)) return;
 
-	read_lock(&tasklist_lock);
-	now = jiffies_to_nsecs(jiffies);
 	if (clone_flags & CLONE_THREAD) {
+		rcu_read_lock();
 		penalty = inherit_burst_tg(parent, now);
+		rcu_read_unlock();
 	} else {
-		if (clone_flags & CLONE_PARENT)
-			parent = parent->real_parent;
+		read_lock(&tasklist_lock);
 		penalty = likely(sched_burst_fork_atavistic) ?
-			inherit_burst_topological(parent, now):
-			inherit_burst_direct(parent, now);
+			inherit_burst_topological(parent, now, clone_flags):
+			inherit_burst_direct(parent, now, clone_flags);
+		read_unlock(&tasklist_lock);
 	}
-	read_unlock(&tasklist_lock);
 
-	struct sched_entity *se = &p->se;
 	revolve_burst_penalty(se);
 	se->burst_penalty = se->prev_burst_penalty =
 		max(se->prev_burst_penalty, penalty);
@@ -286,7 +324,7 @@ void sched_clone_bore(
 	se->group_burst.timestamp = 0;
 }
 
-void init_task_bore(struct task_struct *p) {
+void reset_task_bore(struct task_struct *p) {
 	p->se.burst_time = 0;
 	p->se.prev_burst_penalty = 0;
 	p->se.curr_burst_penalty = 0;
@@ -298,7 +336,8 @@ void init_task_bore(struct task_struct *
 
 void __init sched_bore_init(void) {
 	printk(KERN_INFO "BORE (Burst-Oriented Response Enhancer) CPU Scheduler modification %s by Masahito Suzuki", SCHED_BORE_VERSION);
-    init_task_bore(&init_task);
+	reset_task_bore(&init_task);
+	init_task_burst_cache_lock(&init_task);
 }
 
 #ifdef CONFIG_SYSCTL
diff -uarp a/kernel/sched/fair.c b/kernel/sched/fair.c
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@ -12911,7 +12911,7 @@ static void switched_from_fair(struct rq
 {
 	p->se.rel_deadline = 0;
 #ifdef CONFIG_SCHED_BORE
-	init_task_bore(p);
+	reset_task_bore(p);
 #endif // CONFIG_SCHED_BORE
 	detach_task_cfs_rq(p);
 }
