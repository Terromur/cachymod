# Update patch for BORE 5.7.3

Mostly resembles BORE 5.7.13
* sched_burst_cache_stop_count is a define directive, same default value
* added rcu_read_lock/unlock statements to reset_task_weights_bore
* the count function iterates over a list safe against removal of list entry
* not added (clone_flags & CLONE_PARENT) check, because want to validate first


diff -uarp a/kernel/sched/bore.c b/kernel/sched/bore.c
--- a/kernel/sched/bore.c
+++ b/kernel/sched/bore.c
@@ -15,7 +15,7 @@ u8   __read_mostly sched_burst_fork_atav
 u8   __read_mostly sched_burst_parity_threshold = 2;
 u8   __read_mostly sched_burst_penalty_offset   = 24;
 uint __read_mostly sched_burst_penalty_scale    = 1280;
-uint __read_mostly sched_burst_cache_lifetime   = 60000000;
+uint __read_mostly sched_burst_cache_lifetime   = 75000000;
 uint __read_mostly sched_deadline_boost_mask    = ENQUEUE_INITIAL
                                                 | ENQUEUE_WAKEUP;
 static int __maybe_unused sixty_four     = 64;
@@ -23,6 +23,7 @@ static int __maybe_unused maxval_u8
 static int __maybe_unused maxval_12_bits = 4095;
 
 #define MAX_BURST_PENALTY (39U <<2)
+#define SCHED_BURST_CACHE_STOP_COUNT 64
 
 static inline u32 log2plus1_u64_u32f8(u64 v) {
 	u32 integral = fls64(v);
@@ -119,13 +120,14 @@ void restart_burst_rescale_deadline(stru
 }
 
 static inline bool task_is_bore_eligible(struct task_struct *p)
-{return p->sched_class == &fair_sched_class;}
+{return p && p->sched_class == &fair_sched_class && !p->exit_state;}
 
 static void reset_task_weights_bore(void) {
 	struct task_struct *task;
 	struct rq *rq;
 	struct rq_flags rf;
 
+	rcu_read_lock();
 	write_lock_irq(&tasklist_lock);
 	for_each_process(task) {
 		if (!task_is_bore_eligible(task)) continue;
@@ -135,6 +137,7 @@ static void reset_task_weights_bore(void
 		rq_unlock_irqrestore(rq, &rf);
 	}
 	write_unlock_irq(&tasklist_lock);
+	rcu_read_unlock();
 }
 
 int sched_bore_update_handler(const struct ctl_table *table, int write,
@@ -148,10 +151,18 @@ int sched_bore_update_handler(const stru
 	return 0;
 }
 
-static u32 count_child_tasks(struct task_struct *p) {
-	struct task_struct *child;
-	u32 cnt = 0;
-	list_for_each_entry(child, &p->children, sibling) {cnt++;}
+#define for_each_child(p, t) \
+	list_for_each_entry_rcu(t, &(p)->children, sibling, \
+		lockdep_is_held(&tasklist_lock))
+
+static inline size_t list_count_nodes_safe_max2(struct list_head *head) {
+	struct list_head *pos, *tmp;
+	size_t cnt = 0;
+
+	list_for_each_safe(pos, tmp, head) {
+		if (2 <= ++cnt) break;
+	}
+
 	return cnt;
 }
 
@@ -170,7 +181,7 @@ static inline void update_child_burst_di
 	u32 cnt = 0, sum = 0;
 	struct task_struct *child;
 
-	list_for_each_entry(child, &p->children, sibling) {
+	for_each_child(p, child) {
 		if (!task_is_bore_eligible(child)) continue;
 		cnt++;
 		sum += child->se.burst_penalty;
@@ -192,13 +203,16 @@ static void update_child_burst_topologic
 	u32 cnt = 0, dcnt = 0, sum = 0;
 	struct task_struct *child, *dec;
 
-	list_for_each_entry(child, &p->children, sibling) {
+	for_each_child(p, child) {
+		if (!task_is_bore_eligible(child)) continue;
 		dec = child;
-		while ((dcnt = count_child_tasks(dec)) == 1)
-			dec = list_first_entry(&dec->children, struct task_struct, sibling);
-		
+
+		while (dec && (dcnt = list_count_nodes_safe_max2(&dec->children)) == 1)
+			dec = list_first_or_null_rcu(&dec->children, struct task_struct, sibling);
+
+		if (!task_is_bore_eligible(dec)) continue;
+
 		if (!dcnt || !depth) {
-			if (!task_is_bore_eligible(dec)) continue;
 			cnt++;
 			sum += dec->se.burst_penalty;
 			continue;
@@ -206,6 +220,7 @@ static void update_child_burst_topologic
 		if (!burst_cache_expired(&dec->se.child_burst, now)) {
 			cnt += dec->se.child_burst.count;
 			sum += (u32)dec->se.child_burst.score * dec->se.child_burst.count;
+			if (SCHED_BURST_CACHE_STOP_COUNT <= cnt) break;
 			continue;
 		}
 		update_child_burst_topological(dec, now, depth - 1, &cnt, &sum);
@@ -220,8 +235,9 @@ static inline u8 inherit_burst_topologic
 	struct task_struct *anc = p;
 	u32 cnt = 0, sum = 0;
 
-	while (anc->real_parent != anc && count_child_tasks(anc) == 1)
-		anc = anc->real_parent;
+	for (struct task_struct *next;
+		anc != (next = anc->real_parent) && list_count_nodes_safe_max2(&anc->children) <= 1;
+		anc = next) {}
 
 	if (burst_cache_expired(&anc->se.child_burst, now))
 		update_child_burst_topological(
@@ -244,7 +260,7 @@ static inline void update_tg_burst(struc
 }
 
 static inline u8 inherit_burst_tg(struct task_struct *p, u64 now) {
-	struct task_struct *parent = p->group_leader;
+	struct task_struct *parent = rcu_dereference(p->group_leader);
 	if (burst_cache_expired(&parent->se.group_burst, now))
 		update_tg_burst(parent, now);
 
@@ -253,16 +269,24 @@ static inline u8 inherit_burst_tg(struct
 
 void sched_clone_bore(
 	struct task_struct *p, struct task_struct *parent, u64 clone_flags) {
+	u64 now;
+	u8 penalty;
+
 	if (!task_is_bore_eligible(p)) return;
 
-	u64 now = ktime_get_ns();
-	read_lock(&tasklist_lock);
-	u8 penalty = (clone_flags & CLONE_THREAD) ?
-		inherit_burst_tg(parent, now) :
-		likely(sched_burst_fork_atavistic) ?
+	if (clone_flags & CLONE_THREAD) {
+		rcu_read_lock();
+		now = jiffies_to_nsecs(jiffies);
+		penalty = inherit_burst_tg(parent, now);
+		rcu_read_unlock();
+	} else {
+		read_lock(&tasklist_lock);
+		now = jiffies_to_nsecs(jiffies);
+		penalty = likely(sched_burst_fork_atavistic) ?
 			inherit_burst_topological(parent, now):
 			inherit_burst_direct(parent, now);
-	read_unlock(&tasklist_lock);
+		read_unlock(&tasklist_lock);
+	}
 
 	struct sched_entity *se = &p->se;
 	revolve_burst_penalty(se);
