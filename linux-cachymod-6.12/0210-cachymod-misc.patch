
sched_ext: Always call put_prev_task() with scx_enabled
https://lore.kernel.org/lkml/20241013173928.20738-1-andrea.righi@linux.dev/T/#u

From: Andrea Righi @ 2024-10-13 17:39 UTC (permalink / raw)
  To: Tejun Heo, David Vernet, Peter Zijlstra, Ingo Molnar, Juri Lelli,
	Vincent Guittot
  Cc: Dietmar Eggemann, Steven Rostedt, Ben Segall, Mel Gorman,
	Valentin Schneider, linux-kernel

With the consolidation of put_prev_task/set_next_task(), we are now
skipping the sched_ext ops.stopping/running() transitions when the
previous and next tasks are the same, see commit 436f3eed5c69 ("sched:
Combine the last put_prev_task() and the first set_next_task()").

While this optimization makes sense in general, it can negatively impact
performance in some user-space schedulers, that expect to handle such
transitions when tasks exhaust their timeslice (see SCX_OPS_ENQ_LAST).

For example, scx_rustland suffers a significant performance regression
(e.g., gaming benchmarks drop from ~60fps to ~10fps).

To fix this, ensure that put_prev_task()/set_next_task() are never
skipped when the scx scheduling class is enabled, allowing the scx class
to handle such transitions.

This change restores the previous behavior, fixing the performance
regression in scx_rustland.

Link: https://github.com/sched-ext/scx/issues/788
Fixes: 7c65ae81ea86 ("sched_ext: Don't call put_prev_task_scx() before picking the next task")
Signed-off-by: Andrea Righi <andrea.righi@linux.dev>
---
 kernel/sched/sched.h | 30 +++++++++++++++---------------
 1 file changed, 15 insertions(+), 15 deletions(-)

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 6085ef50febf..44d736e49d06 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -2470,21 +2470,6 @@ __put_prev_set_next_dl_server(struct rq *rq,
 	rq->dl_server = NULL;
 }
 
-static inline void put_prev_set_next_task(struct rq *rq,
-					  struct task_struct *prev,
-					  struct task_struct *next)
-{
-	WARN_ON_ONCE(rq->curr != prev);
-
-	__put_prev_set_next_dl_server(rq, prev, next);
-
-	if (next == prev)
-		return;
-
-	prev->sched_class->put_prev_task(rq, prev, next);
-	next->sched_class->set_next_task(rq, next, true);
-}
-
 /*
  * Helper to define a sched_class instance; each one is placed in a separate
  * section which is ordered by the linker script:
@@ -2523,6 +2508,21 @@ DECLARE_STATIC_KEY_FALSE(__scx_switched_all);	/* all fair class tasks on SCX */
 #define scx_switched_all()	false
 #endif /* !CONFIG_SCHED_CLASS_EXT */
 
+static inline void put_prev_set_next_task(struct rq *rq,
+					  struct task_struct *prev,
+					  struct task_struct *next)
+{
+	WARN_ON_ONCE(rq->curr != prev);
+
+	__put_prev_set_next_dl_server(rq, prev, next);
+
+	if (next == prev && !scx_enabled())
+		return;
+
+	prev->sched_class->put_prev_task(rq, prev, next);
+	next->sched_class->set_next_task(rq, next, true);
+}
+
 /*
  * Iterate only active classes. SCX can take over all fair tasks or be
  * completely disabled. If the former, skip fair. If the latter, skip SCX.
-- 
2.47.0

Move PREEMPT_RT before PREEMPT in vermagic.
https://lore.kernel.org/all/fedd0c6b-a870-4b8f-8f52-6e11df936678@suse.com/

diff --git a/include/linux/vermagic.h b/include/linux/vermagic.h
--- a/include/linux/vermagic.h
+++ b/include/linux/vermagic.h
@@ -15,10 +15,10 @@
 #else
 #define MODULE_VERMAGIC_SMP ""
 #endif
-#ifdef CONFIG_PREEMPT_BUILD
-#define MODULE_VERMAGIC_PREEMPT "preempt "
-#elif defined(CONFIG_PREEMPT_RT)
+#ifdef CONFIG_PREEMPT_RT
 #define MODULE_VERMAGIC_PREEMPT "preempt_rt "
+#elif defined(CONFIG_PREEMPT_BUILD)
+#define MODULE_VERMAGIC_PREEMPT "preempt "
 #else
 #define MODULE_VERMAGIC_PREEMPT ""
 #endif
-- 
2.34.1

sched_idle: Optimize the generic idle loop by removing needless memory barrier
https://lore.kernel.org/all/20241009093745.9504-1-quic_zhonhan@quicinc.com/

From: Zhongqiu Han <quic_zhonhan@quicinc.com>
To: <mingo@redhat.com>, <peterz@infradead.org>,
	<juri.lelli@redhat.com>, <vincent.guittot@linaro.org>,
	<dietmar.eggemann@arm.com>, <rostedt@goodmis.org>,
	<bsegall@google.com>, <mgorman@suse.de>, <vschneid@redhat.com>
Cc: <linux-kernel@vger.kernel.org>, <quic_zhonhan@quicinc.com>
Subject: [PATCH RESEND] sched: idle: Optimize the generic idle loop by removing needless memory barrier
Date: Wed, 9 Oct 2024 17:37:45 +0800	[thread overview]
Message-ID: <20241009093745.9504-1-quic_zhonhan@quicinc.com> (raw)

The memory barrier rmb() in generic idle loop do_idle() function is not
needed, it doesn't order any load instruction, just remove it as needless
rmb() can cause performance impact.

The rmb() was introduced by the tglx/history.git commit f2f1b44c75c4
("[PATCH] Remove RCU abuse in cpu_idle()") to order the loads between
cpu_idle_map and pm_idle. It pairs with wmb() in function cpu_idle_wait().

And then with the removal of cpu_idle_state in function cpu_idle() and
wmb() in function cpu_idle_wait() in commit 783e391b7b5b ("x86: Simplify
cpu_idle_wait"), rmb() no longer has a reason to exist.

After that, commit d16699123434 ("idle: Implement generic idle function")
implemented a generic idle function cpu_idle_loop() which resembles the
functionality found in arch/. And it retained the rmb() in generic idle
loop in file kernel/cpu/idle.c.

And at last, commit cf37b6b48428 ("sched/idle: Move cpu/idle.c to
sched/idle.c") moved cpu/idle.c to sched/idle.c. And commit c1de45ca831a
("sched/idle: Add support for tasks that inject idle") renamed function
cpu_idle_loop() to do_idle().

Signed-off-by: Zhongqiu Han <quic_zhonhan@quicinc.com>
History Tree: https://git.kernel.org/pub/scm/linux/kernel/git/tglx/history.git
---
 kernel/sched/idle.c | 1 -
 1 file changed, 1 deletion(-)

diff --git a/kernel/sched/idle.c b/kernel/sched/idle.c
index d2f096bb274c..ab911d1335ba 100644
--- a/kernel/sched/idle.c
+++ b/kernel/sched/idle.c
@@ -271,7 +271,6 @@ static void do_idle(void)
 	tick_nohz_idle_enter();
 
 	while (!need_resched()) {
-		rmb();
 
 		/*
 		 * Interrupts shouldn't be re-enabled from that point on until
-- 
2.25.1

Switch to gnu17, a GCC bug-fix version of the C11 standard.

What is C17 and what changes have been made to the language?
https://stackoverflow.com/questions/47529854/

Signed-off-by: Mario Roy <...>

diff -uarp a/arch/x86/Makefile b/arch/x86/Makefile
--- a/arch/x86/Makefile
+++ b/arch/x86/Makefile
@@ -47,7 +47,7 @@ endif
 
 # How to compile the 16-bit code.  Note we always compile for -march=i386;
 # that way we can complain to the user if the CPU is insufficient.
-REALMODE_CFLAGS	:= -std=gnu11 -m16 -g -Os -DDISABLE_BRANCH_PROFILING -D__DISABLE_EXPORTS \
+REALMODE_CFLAGS	:= -std=gnu17 -m16 -g -Os -DDISABLE_BRANCH_PROFILING -D__DISABLE_EXPORTS \
 		   -Wall -Wstrict-prototypes -march=i386 -mregparm=3 \
 		   -fno-strict-aliasing -fomit-frame-pointer -fno-pic \
 		   -mno-mmx -mno-sse $(call cc-option,-fcf-protection=none)
diff -uarp a/Makefile b/Makefile
--- a/Makefile
+++ b/Makefile
@@ -462,7 +462,7 @@ HOSTRUSTC = rustc
 HOSTPKG_CONFIG	= pkg-config
 
 KBUILD_USERHOSTCFLAGS := -Wall -Wmissing-prototypes -Wstrict-prototypes \
-			 -O2 -fomit-frame-pointer -std=gnu11
+			 -O2 -fomit-frame-pointer -std=gnu17
 KBUILD_USERCFLAGS  := $(KBUILD_USERHOSTCFLAGS) $(USERCFLAGS)
 KBUILD_USERLDFLAGS := $(USERLDFLAGS)
 
@@ -575,7 +575,7 @@ LINUXINCLUDE    := \
 KBUILD_AFLAGS   := -D__ASSEMBLY__ -fno-PIE
 
 KBUILD_CFLAGS :=
-KBUILD_CFLAGS += -std=gnu11
+KBUILD_CFLAGS += -std=gnu17
 KBUILD_CFLAGS += -fshort-wchar
 KBUILD_CFLAGS += -funsigned-char
 KBUILD_CFLAGS += -fno-common
-- 
2.30.1

Curated patches from XanMod Linux
https://gitlab.com/xanmod/linux-patches

# 0003-kbuild-Remove-GCC-minimal-function-alignment.patch

From 67e174927705e71b0d254ab6fab5af40193376a4 Mon Sep 17 00:00:00 2001
From: Alexandre Frade <kernel@xanmod.org>
Date: Sat, 31 Aug 2024 16:57:41 +0000
Subject: [PATCH 03/18] kbuild: Remove GCC minimal function alignment

Signed-off-by: Alexandre Frade <kernel@xanmod.org>
---
 Makefile                       |  7 -------
 arch/Kconfig                   | 12 ------------
 include/linux/compiler_types.h | 10 +++++-----
 3 files changed, 5 insertions(+), 24 deletions(-)

diff --git a/Makefile b/Makefile
index 59a938a0c335..1f9ef37558df 100644
--- a/Makefile
+++ b/Makefile
@@ -975,15 +975,8 @@ export CC_FLAGS_FPU
 export CC_FLAGS_NO_FPU
 
 ifneq ($(CONFIG_FUNCTION_ALIGNMENT),0)
-# Set the minimal function alignment. Use the newer GCC option
-# -fmin-function-alignment if it is available, or fall back to -falign-funtions.
-# See also CONFIG_CC_HAS_SANE_FUNCTION_ALIGNMENT.
-ifdef CONFIG_CC_HAS_MIN_FUNCTION_ALIGNMENT
-KBUILD_CFLAGS += -fmin-function-alignment=$(CONFIG_FUNCTION_ALIGNMENT)
-else
 KBUILD_CFLAGS += -falign-functions=$(CONFIG_FUNCTION_ALIGNMENT)
 endif
-endif
 
 # arch Makefile may override CC so keep this after arch Makefile is included
 NOSTDINC_FLAGS += -nostdinc
diff --git a/arch/Kconfig b/arch/Kconfig
index bd9f095d69fa..7c1f69a6dc37 100644
--- a/arch/Kconfig
+++ b/arch/Kconfig
@@ -1667,18 +1667,6 @@ config FUNCTION_ALIGNMENT
 	default 4 if FUNCTION_ALIGNMENT_4B
 	default 0
 
-config CC_HAS_MIN_FUNCTION_ALIGNMENT
-	# Detect availability of the GCC option -fmin-function-alignment which
-	# guarantees minimal alignment for all functions, unlike
-	# -falign-functions which the compiler ignores for cold functions.
-	def_bool $(cc-option, -fmin-function-alignment=8)
-
-config CC_HAS_SANE_FUNCTION_ALIGNMENT
-	# Set if the guaranteed alignment with -fmin-function-alignment is
-	# available or extra care is required in the kernel. Clang provides
-	# strict alignment always, even with -falign-functions.
-	def_bool CC_HAS_MIN_FUNCTION_ALIGNMENT || CC_IS_CLANG
-
 config ARCH_NEED_CMPXCHG_1_EMU
 	bool
 
diff --git a/include/linux/compiler_types.h b/include/linux/compiler_types.h
index 1a957ea2f4fe..972bd5f84766 100644
--- a/include/linux/compiler_types.h
+++ b/include/linux/compiler_types.h
@@ -99,17 +99,17 @@ static inline void __chk_io_ptr(const volatile void __iomem *ptr) { }
  *   gcc: https://gcc.gnu.org/onlinedocs/gcc/Label-Attributes.html#index-cold-label-attribute
  *
  * When -falign-functions=N is in use, we must avoid the cold attribute as
- * GCC drops the alignment for cold functions. Worse, GCC can implicitly mark
- * callees of cold functions as cold themselves, so it's not sufficient to add
- * __function_aligned here as that will not ensure that callees are correctly
- * aligned.
+ * contemporary versions of GCC drop the alignment for cold functions. Worse,
+ * GCC can implicitly mark callees of cold functions as cold themselves, so
+ * it's not sufficient to add __function_aligned here as that will not ensure
+ * that callees are correctly aligned.
  *
  * See:
  *
  *   https://lore.kernel.org/lkml/Y77%2FqVgvaJidFpYt@FVFF77S0Q05N
  *   https://gcc.gnu.org/bugzilla/show_bug.cgi?id=88345#c9
  */
-#if defined(CONFIG_CC_HAS_SANE_FUNCTION_ALIGNMENT) || (CONFIG_FUNCTION_ALIGNMENT == 0)
+#if !defined(CONFIG_CC_IS_GCC) || (CONFIG_FUNCTION_ALIGNMENT == 0)
 #define __cold				__attribute__((__cold__))
 #else
 #define __cold
-- 
2.45.2

# 0006-XANMOD-block-mq-deadline-Increase-write-priority-to-.patch

From d23f0554f1b381f082dc81a6f3c523b90043b941 Mon Sep 17 00:00:00 2001
From: Alexandre Frade <kernel@xanmod.org>
Date: Wed, 11 May 2022 18:56:51 +0000
Subject: [PATCH 06/18] XANMOD: block/mq-deadline: Increase write priority to
 improve responsiveness

Signed-off-by: Alexandre Frade <kernel@xanmod.org>
---
 block/mq-deadline.c | 7 +++++--
 1 file changed, 5 insertions(+), 2 deletions(-)

diff --git a/block/mq-deadline.c b/block/mq-deadline.c
index acdc28756d9d..9d7f63ebc000 100644
--- a/block/mq-deadline.c
+++ b/block/mq-deadline.c
@@ -4,6 +4,9 @@
  *  for the blk-mq scheduling framework
  *
  *  Copyright (C) 2016 Jens Axboe <axboe@kernel.dk>
+ *
+ *  Tunes for responsiveness by Alexandre Frade
+ *  (C) 2022 Alexandre Frade <kernel@xanmod.org>
  */
 #include <linux/kernel.h>
 #include <linux/fs.h>
@@ -28,13 +31,13 @@
  * See Documentation/block/deadline-iosched.rst
  */
 static const int read_expire = HZ / 2;  /* max time before a read is submitted. */
-static const int write_expire = 5 * HZ; /* ditto for writes, these limits are SOFT! */
+static const int write_expire = HZ;     /* ditto for writes, these limits are SOFT! */
 /*
  * Time after which to dispatch lower priority requests even if higher
  * priority requests are pending.
  */
 static const int prio_aging_expire = 10 * HZ;
-static const int writes_starved = 2;    /* max times reads can starve a write */
+static const int writes_starved = 1;    /* max times reads can starve a write */
 static const int fifo_batch = 16;       /* # of sequential requests treated as one
 				     by the above parameters. For throughput. */
 
-- 
2.45.2

# 0007-XANMOD-block-mq-deadline-Disable-front_merges-by-def.patch

From 8c3035b22be106d8659d85c2651e589f53e89cc5 Mon Sep 17 00:00:00 2001
From: Alexandre Frade <kernel@xanmod.org>
Date: Thu, 6 Jan 2022 16:59:01 +0000
Subject: [PATCH 07/18] XANMOD: block/mq-deadline: Disable front_merges by
 default

Signed-off-by: Alexandre Frade <kernel@xanmod.org>
---
 block/mq-deadline.c | 2 +-
 1 file changed, 1 insertion(+), 1 deletion(-)

diff --git a/block/mq-deadline.c b/block/mq-deadline.c
index 9d7f63ebc000..bf67b420b645 100644
--- a/block/mq-deadline.c
+++ b/block/mq-deadline.c
@@ -600,7 +600,7 @@ static int dd_init_sched(struct request_queue *q, struct elevator_type *e)
 	dd->fifo_expire[DD_READ] = read_expire;
 	dd->fifo_expire[DD_WRITE] = write_expire;
 	dd->writes_starved = writes_starved;
-	dd->front_merges = 1;
+	dd->front_merges = 0;
 	dd->last_dir = DD_WRITE;
 	dd->fifo_batch = fifo_batch;
 	dd->prio_aging_expire = prio_aging_expire;
-- 
2.45.2

# 0008-XANMOD-block-Set-rq_affinity-to-force-complete-I-O-r.patch

From 3d1e3f450e9ca926a899a0502fd34df6d483efae Mon Sep 17 00:00:00 2001
From: Alexandre Frade <kernel@xanmod.org>
Date: Mon, 16 Sep 2024 15:36:01 +0000
Subject: [PATCH 08/18] XANMOD: block: Set rq_affinity to force complete I/O
 requests on same CPU

Signed-off-by: Alexandre Frade <kernel@xanmod.org>
---
 include/linux/blkdev.h | 3 ++-
 1 file changed, 2 insertions(+), 1 deletion(-)

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 50c3b959da28..e291cd33edcf 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -605,7 +605,8 @@ enum {
 	QUEUE_FLAG_MAX
 };
 
-#define QUEUE_FLAG_MQ_DEFAULT	(1UL << QUEUE_FLAG_SAME_COMP)
+#define QUEUE_FLAG_MQ_DEFAULT	((1UL << QUEUE_FLAG_SAME_COMP) |		\
+				 (1UL << QUEUE_FLAG_SAME_FORCE))
 
 void blk_queue_flag_set(unsigned int flag, struct request_queue *q);
 void blk_queue_flag_clear(unsigned int flag, struct request_queue *q);
-- 
2.45.2

# 0009-XANMOD-blk-wbt-Set-wbt_default_latency_nsec-to-2msec.patch

From 80e2bd58a4f13d1a946c6616e18d124b1291f2a7 Mon Sep 17 00:00:00 2001
From: Alexandre Frade <kernel@xanmod.org>
Date: Mon, 15 Jul 2024 04:50:34 +0000
Subject: [PATCH 09/18] XANMOD: blk-wbt: Set wbt_default_latency_nsec() to
 2msec

Signed-off-by: Alexandre Frade <kernel@xanmod.org>
---
 block/blk-wbt.c | 10 ++--------
 1 file changed, 2 insertions(+), 8 deletions(-)

diff --git a/block/blk-wbt.c b/block/blk-wbt.c
index 6dfc659d22e2..4cf65ad7c953 100644
--- a/block/blk-wbt.c
+++ b/block/blk-wbt.c
@@ -730,14 +730,8 @@ EXPORT_SYMBOL_GPL(wbt_enable_default);
 
 u64 wbt_default_latency_nsec(struct request_queue *q)
 {
-	/*
-	 * We default to 2msec for non-rotational storage, and 75msec
-	 * for rotational storage.
-	 */
-	if (blk_queue_nonrot(q))
-		return 2000000ULL;
-	else
-		return 75000000ULL;
+	/* XanMod defaults to 2msec for any type of storage */
+	return 2000000ULL;
 }
 
 static int wbt_data_dir(const struct request *rq)
-- 
2.45.2

# 0011-XANMOD-dcache-cache_pressure-50-decreases-the-rate-a.patch

From a047058c64e9d75db8e714a8c1202057920e21c7 Mon Sep 17 00:00:00 2001
From: Alexandre Frade <kernel@xanmod.org>
Date: Mon, 29 Jan 2018 16:59:22 +0000
Subject: [PATCH 11/18] XANMOD: dcache: cache_pressure = 50 decreases the rate
 at which VFS caches are reclaimed

Signed-off-by: Alexandre Frade <kernel@xanmod.org>
---
 fs/dcache.c | 2 +-
 1 file changed, 1 insertion(+), 1 deletion(-)

diff --git a/fs/dcache.c b/fs/dcache.c
index 0f6b16ba30d0..ad80bfd6244f 100644
--- a/fs/dcache.c
+++ b/fs/dcache.c
@@ -73,7 +73,7 @@
  * If no ancestor relationship:
  * arbitrary, since it's serialized on rename_lock
  */
-int sysctl_vfs_cache_pressure __read_mostly = 100;
+int sysctl_vfs_cache_pressure __read_mostly = 50;
 EXPORT_SYMBOL_GPL(sysctl_vfs_cache_pressure);
 
 __cacheline_aligned_in_smp DEFINE_SEQLOCK(rename_lock);
-- 
2.45.2

# 0016-XANMOD-lib-kconfig.debug-disable-default-SYMBOLIC_ER.patch

From 444f831f229a418b4865d11940b3987f55ab151f Mon Sep 17 00:00:00 2001
From: Alexandre Frade <kernel@xanmod.org>
Date: Mon, 16 Sep 2024 08:09:56 +0000
Subject: [PATCH 16/18] XANMOD: lib/kconfig.debug: disable default
 SYMBOLIC_ERRNAME and DEBUG_BUGVERBOSE

Signed-off-by: Alexandre Frade <kernel@xanmod.org>
---
 fs/bcachefs/Kconfig | 1 -
 lib/Kconfig.debug   | 4 ++--
 2 files changed, 2 insertions(+), 3 deletions(-)

diff --git a/fs/bcachefs/Kconfig b/fs/bcachefs/Kconfig
index 5bac803ea367..773befb92af1 100644
--- a/fs/bcachefs/Kconfig
+++ b/fs/bcachefs/Kconfig
@@ -23,7 +23,6 @@ config BCACHEFS_FS
 	select XOR_BLOCKS
 	select XXHASH
 	select SRCU
-	select SYMBOLIC_ERRNAME
 	help
 	The bcachefs filesystem - a modern, copy on write filesystem, with
 	support for multiple devices, compression, checksumming, etc.
diff --git a/lib/Kconfig.debug b/lib/Kconfig.debug
index 7312ae7c3cc5..78f629791bdf 100644
--- a/lib/Kconfig.debug
+++ b/lib/Kconfig.debug
@@ -190,7 +190,7 @@ config DYNAMIC_DEBUG_CORE
 
 config SYMBOLIC_ERRNAME
 	bool "Support symbolic error names in printf"
-	default y if PRINTK
+	default n
 	help
 	  If you say Y here, the kernel's printf implementation will
 	  be able to print symbolic error names such as ENOSPC instead
@@ -200,7 +200,7 @@ config SYMBOLIC_ERRNAME
 config DEBUG_BUGVERBOSE
 	bool "Verbose BUG() reporting (adds 70K)" if DEBUG_KERNEL && EXPERT
 	depends on BUG && (GENERIC_BUG || HAVE_DEBUG_BUGVERBOSE)
-	default y
+	default n
 	help
 	  Say Y here to make BUG() panics output the file name and line number
 	  of the BUG call as well as the EIP and oops trace.  This aids
-- 
2.45.2

# 0002-ZEN-dm-crypt-Disable-workqueues-for-crypto-ops.patch

From deef13431e620a3637755f5fb1a671661050e755 Mon Sep 17 00:00:00 2001
From: Steven Barrett <steven@liquorix.net>
Date: Sat, 21 May 2022 15:15:09 -0500
Subject: [PATCH 2/2] ZEN: dm-crypt: Disable workqueues for crypto ops

Queueing in dm-crypt for crypto operations reduces performance on modern
systems.  As discussed in an article from Cloudflare, they discovered
that queuing was introduced because the crypto subsystem used to be
synchronous.  Since it's now asynchronous, we get double queueing when
using the subsystem through dm-crypt.  This is obviously undesirable and
reduces throughput and increases latency.

Fixes: https://github.com/zen-kernel/zen-kernel/issues/282
Signed-off-by: Alexandre Frade <kernel@xanmod.org>
---
 drivers/md/dm-crypt.c | 3 +++
 1 file changed, 3 insertions(+)

diff --git a/drivers/md/dm-crypt.c b/drivers/md/dm-crypt.c
index 1ae2c71bb383..55d6b4806677 100644
--- a/drivers/md/dm-crypt.c
+++ b/drivers/md/dm-crypt.c
@@ -3315,6 +3315,9 @@ static int crypt_ctr(struct dm_target *ti, unsigned int argc, char **argv)
 			goto bad;
 	}
 
+	set_bit(DM_CRYPT_NO_READ_WORKQUEUE, &cc->flags);
+	set_bit(DM_CRYPT_NO_WRITE_WORKQUEUE, &cc->flags);
+
 	ret = crypt_ctr_cipher(ti, argv[0], argv[1]);
 	if (ret < 0)
 		goto bad;
-- 
2.45.2

Curated patches from SUNLIGHT Linux
https://github.com/sunlightlinux/linux-sunlight

x86/tsc: Use rdtsc_ordered() when RDTSCP or LFENCE_RDTSC are supported

Signed-off-by: Mario Limonciello <mario.limonciello@amd.com>
Signed-off-by: Ionut Nechita <ionut_n2001@yahoo.com>

diff --git a/arch/x86/include/asm/tsc.h b/arch/x86/include/asm/tsc.h
--- a/arch/x86/include/asm/tsc.h
+++ b/arch/x86/include/asm/tsc.h
@@ -24,6 +24,9 @@ static inline cycles_t get_cycles(void)
 	if (!IS_ENABLED(CONFIG_X86_TSC) &&
 	    !cpu_feature_enabled(X86_FEATURE_TSC))
 		return 0;
+	if (cpu_feature_enabled(X86_FEATURE_LFENCE_RDTSC) ||
+	    cpu_feature_enabled(X86_FEATURE_RDTSCP))
+		return rdtsc_ordered();
 	return rdtsc();
 }
 #define get_cycles get_cycles
-- 
2.43.0

rcu: Change sched_setscheduler_nocheck() calls to SCHED_RR policy

Signed-off-by: Alexandre Frade <kernel@xanmod.org>
Signed-off-by: Ionut Nechita <ionut_n2001@yahoo.com>

diff --git a/Documentation/admin-guide/kernel-parameters.txt b/Documentation/admin-guide/kernel-parameters.txt
--- a/Documentation/admin-guide/kernel-parameters.txt
+++ b/Documentation/admin-guide/kernel-parameters.txt
@@ -5070,7 +5070,7 @@
 			overwritten.
 
 	rcutree.kthread_prio= 	 [KNL,BOOT]
-			Set the SCHED_FIFO priority of the RCU per-CPU
+			Set the SCHED_RR priority of the RCU per-CPU
 			kthreads (rcuc/N). This value is also used for
 			the priority of the RCU boost threads (rcub/N)
 			and for the RCU grace-period kthreads (rcu_bh,
diff --git a/kernel/rcu/Kconfig b/kernel/rcu/Kconfig
--- a/kernel/rcu/Kconfig
+++ b/kernel/rcu/Kconfig
@@ -281,9 +281,9 @@ config RCU_NOCB_CPU_CB_BOOST
 	depends on RCU_NOCB_CPU && RCU_BOOST
 	default y if PREEMPT_RT
 	help
-	  Use this option to invoke offloaded callbacks as SCHED_FIFO
+	  Use this option to invoke offloaded callbacks as SCHED_RR
 	  to avoid starvation by heavy SCHED_OTHER background load.
-	  Of course, running as SCHED_FIFO during callback floods will
+	  Of course, running as SCHED_RR during callback floods will
 	  cause the rcuo[ps] kthreads to monopolize the CPU for hundreds
 	  of milliseconds or more.  Therefore, when enabling this option,
 	  it is your responsibility to ensure that latency-sensitive
diff --git a/kernel/rcu/rcutorture.c b/kernel/rcu/rcutorture.c
--- a/kernel/rcu/rcutorture.c
+++ b/kernel/rcu/rcutorture.c
@@ -2439,12 +2439,12 @@ static int rcutorture_booster_init(unsigned int cpu)
 		t = per_cpu(ksoftirqd, cpu);
 		WARN_ON_ONCE(!t);
 		sp.sched_priority = 2;
-		sched_setscheduler_nocheck(t, SCHED_FIFO, &sp);
+		sched_setscheduler_nocheck(t, SCHED_RR, &sp);
 #ifdef CONFIG_PREEMPT_RT
 		t = per_cpu(timersd, cpu);
 		WARN_ON_ONCE(!t);
 		sp.sched_priority = 2;
-		sched_setscheduler_nocheck(t, SCHED_FIFO, &sp);
+		sched_setscheduler_nocheck(t, SCHED_RR, &sp);
 #endif
 	}
 
diff --git a/kernel/rcu/tree.c b/kernel/rcu/tree.c
--- a/kernel/rcu/tree.c
--- b/kernel/rcu/tree.c
@@ -4949,7 +4949,7 @@ static void __init rcu_start_exp_gp_kworker(void)
 	}
 
 	if (IS_ENABLED(CONFIG_RCU_EXP_KTHREAD))
-		sched_setscheduler_nocheck(rcu_exp_gp_kworker->task, SCHED_FIFO, &param);
+		sched_setscheduler_nocheck(rcu_exp_gp_kworker->task, SCHED_RR, &param);
 }
 
 static void rcu_spawn_rnp_kthreads(struct rcu_node *rnp)
@@ -5373,7 +5373,7 @@ static int __init rcu_spawn_gp_kthread(void)
 		return 0;
 	if (kthread_prio) {
 		sp.sched_priority = kthread_prio;
-		sched_setscheduler_nocheck(t, SCHED_FIFO, &sp);
+		sched_setscheduler_nocheck(t, SCHED_RR, &sp);
 	}
 	rnp = rcu_get_root();
 	raw_spin_lock_irqsave_rcu_node(rnp, flags);
diff --git a/kernel/rcu/tree_nocb.h b/kernel/rcu/tree_nocb.h
--- a/kernel/rcu/tree_nocb.h
+++ b/kernel/rcu/tree_nocb.h
@@ -1404,7 +1404,7 @@ static void rcu_spawn_cpu_nocb_kthread(int cpu)
 		}
 		WRITE_ONCE(rdp_gp->nocb_gp_kthread, t);
 		if (kthread_prio)
-			sched_setscheduler_nocheck(t, SCHED_FIFO, &sp);
+			sched_setscheduler_nocheck(t, SCHED_RR, &sp);
 	}
 	mutex_unlock(&rdp_gp->nocb_gp_kthread_mutex);
 
@@ -1420,7 +1420,7 @@ static void rcu_spawn_cpu_nocb_kthread(int cpu)
 		kthread_park(t);
 
 	if (IS_ENABLED(CONFIG_RCU_NOCB_CPU_CB_BOOST) && kthread_prio)
-		sched_setscheduler_nocheck(t, SCHED_FIFO, &sp);
+		sched_setscheduler_nocheck(t, SCHED_RR, &sp);
 
 	WRITE_ONCE(rdp->nocb_cb_kthread, t);
 	WRITE_ONCE(rdp->nocb_gp_kthread, rdp_gp->nocb_gp_kthread);
diff --git a/kernel/rcu/tree_plugin.h b/kernel/rcu/tree_plugin.h
--- a/kernel/rcu/tree_plugin.h
+++ b/kernel/rcu/tree_plugin.h
@@ -1020,7 +1020,7 @@ static void rcu_cpu_kthread_setup(unsigned int cpu)
 	struct sched_param sp;
 
 	sp.sched_priority = kthread_prio;
-	sched_setscheduler_nocheck(current, SCHED_FIFO, &sp);
+	sched_setscheduler_nocheck(current, SCHED_RR, &sp);
 #endif /* #ifdef CONFIG_RCU_BOOST */
 
 	WRITE_ONCE(rdp->rcuc_activity, jiffies);
@@ -1218,7 +1218,7 @@ static void rcu_spawn_one_boost_kthread(struct rcu_node *rnp)
 	rnp->boost_kthread_task = t;
 	raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
 	sp.sched_priority = kthread_prio;
-	sched_setscheduler_nocheck(t, SCHED_FIFO, &sp);
+	sched_setscheduler_nocheck(t, SCHED_RR, &sp);
 	wake_up_process(t); /* get to TASK_INTERRUPTIBLE quickly. */
 }
 
-- 
2.43.0

Change default value for wq_cpu_intensive_thresh_us

Description:
 - 10ms -> 30ms
 - By using new processors it helps to have
   a higher threshold for thresh

Bug: N/A
Signed-off-by: Ionut Nechita <ionut_n2001@yahoo.com>

diff --git a/kernel/workqueue.c b/kernel/workqueue.c
--- a/kernel/workqueue.c
+++ b/kernel/workqueue.c
@@ -7827,20 +7827,20 @@ static void __init wq_cpu_intensive_thresh_init(void)
 		return;
 
 	/*
-	 * The default of 10ms is derived from the fact that most modern (as of
-	 * 2023) processors can do a lot in 10ms and that it's just below what
+	 * The default of 30ms is derived from the fact that most modern (as of
+	 * 2023) processors can do a lot in 30ms and that it's just below what
 	 * most consider human-perceivable. However, the kernel also runs on a
 	 * lot slower CPUs including microcontrollers where the threshold is way
 	 * too low.
 	 *
-	 * Let's scale up the threshold upto 1 second if BogoMips is below 4000.
+	 * Let's scale up the threshold upto 3 second if BogoMips is below 4000.
 	 * This is by no means accurate but it doesn't have to be. The mechanism
 	 * is still useful even when the threshold is fully scaled up. Also, as
 	 * the reports would usually be applicable to everyone, some machines
 	 * operating on longer thresholds won't significantly diminish their
 	 * usefulness.
 	 */
-	thresh = 10 * USEC_PER_MSEC;
+	thresh = 30 * USEC_PER_MSEC;
 
 	/* see init/calibrate.c for lpj -> BogoMIPS calculation */
 	bogo = max_t(unsigned long, loops_per_jiffy / 500000 * HZ, 1);
-- 
2.43.0

Minor change BLK_DEV_RUST_NULL with default n

Description:
 -  Minor change BLK_DEV_RUST_NULL with default n

Signed-off-by: Ionut Nechita <ionut_n2001@yahoo.com>

diff --git a/drivers/block/Kconfig b/drivers/block/Kconfig
--- a/drivers/block/Kconfig
+++ b/drivers/block/Kconfig
@@ -355,8 +355,9 @@ config VIRTIO_BLK
           QEMU based VMMs (like KVM or Xen).  Say Y or M.
 
 config BLK_DEV_RUST_NULL
-	tristate "Rust null block driver (Experimental)"
+	bool "Rust null block driver (Experimental)"
 	depends on RUST
+	default n
 	help
 	  This is the Rust implementation of the null block driver. For now it
 	  is only a minimal stub.
-- 
2.43.0

