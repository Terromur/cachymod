BORE 5.9.4 update:
Pass fork.c p->start_time as "now" argument to sched_clone_bore()
Optimize the count_children_max2() function
Signed-off-by: Mario Roy <...>

diff -uarp a/include/linux/sched/bore.h b/include/linux/sched/bore.h
--- a/include/linux/sched/bore.h
+++ b/include/linux/sched/bore.h
@@ -29,7 +29,7 @@ extern int sched_bore_update_handler(con
 	void __user *buffer, size_t *lenp, loff_t *ppos);
 
 extern void sched_clone_bore(
-	struct task_struct *p, struct task_struct *parent, u64 clone_flags);
+	struct task_struct *p, struct task_struct *parent, u64 clone_flags, u64 now);
 
 extern void reset_task_bore(struct task_struct *p);
 extern void sched_bore_init(void);
diff -uarp a/kernel/fork.c b/kernel/fork.c
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -2506,10 +2506,6 @@ __latent_entropy struct task_struct *cop
 	if (retval)
 		goto bad_fork_cancel_cgroup;
 
-#ifdef CONFIG_SCHED_BORE
-	if (likely(p->pid))
-		sched_clone_bore(p, current, clone_flags);
-#endif // CONFIG_SCHED_BORE
 	/*
 	 * From this point on we must avoid any synchronous user-space
 	 * communication until we take the tasklist-lock. In particular, we do
@@ -2521,6 +2517,11 @@ __latent_entropy struct task_struct *cop
 	p->start_time = ktime_get_ns();
 	p->start_boottime = ktime_get_boottime_ns();
 
+#ifdef CONFIG_SCHED_BORE
+	if (likely(p->pid))
+		sched_clone_bore(p, current, clone_flags, p->start_time);
+#endif // CONFIG_SCHED_BORE
+
 	/*
 	 * Make it visible to the rest of the system, but dont wake it up yet.
 	 * Need tasklist lock for parent etc handling!
diff -uarp a/kernel/sched/bore.c b/kernel/sched/bore.c
--- a/kernel/sched/bore.c
+++ b/kernel/sched/bore.c
@@ -154,11 +154,9 @@ int sched_bore_update_handler(const stru
 #define for_each_child(p, t) \
 	list_for_each_entry(t, &(p)->children, sibling)
 
-static u32 count_children_max2(struct task_struct *p) {
-	u32 cnt = 0;
-	struct task_struct *child;
-	for_each_child(p, child) {if (2 <= ++cnt) break;}
-	return cnt;
+static u32 count_children_max2(struct list_head *head) {
+	struct list_head *next = head->next;
+	return (next == head) ? 0 : (next->next == head) ? 1 : 2;
 }
 
 static inline void init_task_burst_cache_lock(struct task_struct *p) {
@@ -215,7 +213,7 @@ static void update_child_burst_topologic
 
 	for_each_child(p, child) {
 		dec = child;
-		while ((dcnt = count_children_max2(dec)) == 1)
+		while ((dcnt = count_children_max2(&dec->children)) == 1)
 			dec = list_first_entry(&dec->children, struct task_struct, sibling);
 		
 		if (!dcnt || !depth) {
@@ -259,7 +257,7 @@ static inline u8 inherit_burst_topologic
 
 	for (struct task_struct *next;
 		 anc != (next = anc->real_parent) &&
-		 	count_children_max2(anc) <= base_child_cnt;) {
+		 	count_children_max2(&anc->children) <= base_child_cnt;) {
 		anc = next;
 		base_child_cnt = 1;
 	}
@@ -299,9 +297,8 @@ static inline u8 inherit_burst_tg(struct
 }
 
 void sched_clone_bore(
-	struct task_struct *p, struct task_struct *parent, u64 clone_flags) {
+	struct task_struct *p, struct task_struct *parent, u64 clone_flags, u64 now) {
 	struct sched_entity *se = &p->se;
-	u64 now;
 	u8 penalty;
 
 	init_task_burst_cache_lock(p);
@@ -310,12 +307,10 @@ void sched_clone_bore(
 
 	if (clone_flags & CLONE_THREAD) {
 		rcu_read_lock();
-		now = jiffies_to_nsecs(jiffies);
 		penalty = inherit_burst_tg(parent, now);
 		rcu_read_unlock();
 	} else {
 		read_lock(&tasklist_lock);
-		now = jiffies_to_nsecs(jiffies);
 		penalty = likely(sched_burst_fork_atavistic) ?
 			inherit_burst_topological(parent, now, clone_flags):
 			inherit_burst_direct(parent, now, clone_flags);
